{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54624bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ipynb_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipynb_3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "#     import findspark\n",
    "#     findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession  \n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    # Configure spark session\n",
    "    spark = SparkSession    .builder    .master('local[2]')    .appName('quake_etl')    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1')    .getOrCreate()\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    # Load the dataset \n",
    "    df_load = spark.read.csv(r\"database.csv\", header=True)\n",
    "    # Preview df_load\n",
    "    df_load.take(1)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    # Drop fields we don't need from df_load\n",
    "    lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "        'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "\n",
    "    df_load = df_load.drop(*lst_dropped_columns)\n",
    "    # Preview df_load\n",
    "    df_load.show(5)\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    # Create a year field and add it to the dataframe\n",
    "    df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "    # Preview df_load\n",
    "    df_load.show(5)\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    # Build the quakes frequency dataframe using the year field and counts for each year\n",
    "    df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "    # Preview df_quake_freq\n",
    "    df_quake_freq.show(5)\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    # Preview df_load schema\n",
    "    df_load.printSchema()\n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    # Cast some fields from string into numeric types\n",
    "    df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "    # Preview df_load\n",
    "    df_load.show(5)\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "\n",
    "    # Preview df_load schema\n",
    "    df_load.printSchema()\n",
    "\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "\n",
    "    # Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "    df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "    df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "\n",
    "    # Join df_max, and df_avg to df_quake_freq\n",
    "    df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "    # Preview df_quake_freq\n",
    "    df_quake_freq.show(5)\n",
    "\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "\n",
    "    # Remove nulls\n",
    "    df_load.dropna()\n",
    "    df_quake_freq.dropna()\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    # Preview dataframes\n",
    "    df_load.show(5)\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "\n",
    "    df_quake_freq.show(5)\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    # Build the tables/collections in mongodb\n",
    "    # Write df_load to mongodb\n",
    "    df_load.write.format('mongo')    .mode('overwrite')    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').save()\n",
    "\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "\n",
    "    # Write df_quake_freq to mongodb\n",
    "    df_quake_freq.write.format('mongo')    .mode('overwrite')    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quake_freq').save()\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c7427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
